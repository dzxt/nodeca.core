> Simple distributed tasks queue, backed with redis.

Typical applying:

- Decouple code execution.
- Postponed task run.
- Periodic task run.
- Load distribution for heavy tasks.

Implementation is simple (compared to standalone services), but enougth for
the most of projects. Advantage of this package is the ease of use.


## API

### new Queue(redis, prefix)

 - **redis** (RedisClient) - redis client instance
 - **prefix** (String) - optional. Redis keys prefix, "queue:" by default

### .registerWorker(options) / .registerWorker(name [, cron], process)

Options:

 - **name** (String) - the worker's name
 - **taskID** (Function) - optional, should return new task id. Needed only for
   creating exclusive tasks, return random value by default, called as: `function (taskData)`
 - **chunksPerInstance** (Number) - optional, available count of parallel chunks in one
   process (Infinity - not restricted), default Infinity
 - **retry** (Number) - optional, number of retry on error, default 2
 - **retryDelay** (Number) - optional, delay in ms after retries, default 60000 ms
 - **timeout** (Number) - optional, `map`, `chunk` and `reduce` execution timeout, default 30000 ms
 - **postponeDelay** (Number) - optional, if postpone is called without delay, delay is assumed to be equal to this
 - **cron** (String) - optional, cron string ("15 \*/6 \* \* \*"), default null
 - **map** (Function) - optional, proxy taskData to single chunk by default,
   called as: `task.map(callback)`
   - **this** (Object) - current task (task data is available as `this.data`)
   - **callback** (Function) - called as: `function (err, chunksData)`
     - **chunksData** (Array) - array of chunks data
 - **process** (Function) - called as: `chunk.process(callback)`
   - **this** (Object) - current chunk (chunk data is available as `this.data`)
   - **callback** (Function) - called as: `function (err, result)`
 - **reduce** (Function) - optional, only call `callback` by default,
   called as: `task.reduce(chunksResult, callback)`
   - **this** (Object) - current task
   - **chunksResult** (Array) - array of chunks results
   - **callback** (Function) - called as: `function (err)`

We use classic map/reduce approach to run tasks. Each task is mapped to chunks,
chunks are executed in parallel on all available queue nodes, then collected data
is reduced to result.

- If map/process/reduce return error or is timed out, this step is repeaded
  after delay. Be careful - don't return error on bad data< create a new task
  to process again. Errors are for global failures (redis, db and so on).
- You can configure number of chunks to run in parallel on each node (with
  `chunksPerInstance`). For example, IO-intensive chunks linke sending emails
   need no limits. CPU-intensive chunks should be limited.


#### `map` function

This function is called when you create a new task, and it's purpose is to split
input data into data chunks. By default it creates a single chunk containing
input data as is.

`this` object represents a task. Structure:

 - **id** (String) - task ID, generated by `worker.taskID` on task creation
 - **data** (Object) - input data
 - **worker** (Object) - a worker instance this task corresponds to (see options above)

#### `process` function

This function is called once for each chunk returned from a `map` function.

`this` object represents a chunk:

 - **id** (String) - randomly generated chunk ID
 - **task** (Object) - a task this chunk corresponds to
   - **id** (String) - task ID, generated by `worker.taskID` on task creation
   - **worker** (Object) - a worker instance this task corresponds to (see options above)

#### `reduce` function

This function is called after all chunks are processed. This is the last step of the task execution.

`this` object represents a task:

 - **id** (String) - task ID, generated by `worker.taskID` on task creation
 - **worker** (Object) - a worker instance this task corresponds to (see options above)


### .push(workerName, taskData, callback)

Run the task immediately.

 - **workerName** (String) - the worker name
 - **taskData** (Object) - optional, the task params
 - **callback** (Function) - called as: `function (err)`

### .postpone(workerName, taskData, delay, callback)

Postpone the task execution.

 - **workerName** (String) - the worker name
 - **taskData** (Object) - optional, the task params, default: `null`
 - **delay** (Number) - optional, delay execution for the given amount of time, default: `worker.postponeDelay`
 - **callback** (Function) - called as: `function (err)`

Note: if you call `.postpone()` with 3 arguments, queue figures out desired signature based on argument type (if it's a number, it's a delay). Thus, if you want to supply a numeric `taskData`, you *must* also specify `delay`.

### .status(taskID, callback)

Get information about task status.

 - **taskID** (String) - full task ID returned from `push` or `postpone` functions
 - **callback** (Function) - called as `function (err, info)`
   - **info** (Object | Null) - task info if task exists, null otherwise
     - **worker** (String) - worker name
     - **state**  (String) - one of "mapping", "aggregating", "reducing" or "postponed"
     - **chunks** (Object) - full chunk IDs by state (optional, only present if task is in "aggregating" state)
       - **pending** (Array) - array of pending chunk IDs
       - **active**  (Array) - array of active chunk IDs
       - **done**    (Array) - array of completed chunk IDs
       - **errored** (Array) - array of failed chunk IDs

### .cancel(taskID, callback)

Cancel the task and remove it from queue. Chunks that started execution
will continue, but their results will be discarded and no new chunks
will be processed.

 - **taskID** (String) - full task ID returned from `push` or `postpone` functions
 - **callback** (Function) - called as: `function (err)`

### .shutdown()

Stop accepting new tasks from queue. Active tasks continue execution.

### .on('error', callback)

`Queue` is an `EventEmitter` instance that only fires `error` event when an error has occured.

