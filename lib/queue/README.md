> Simple distributed tasks queue, backed with redis.

Typical applying:

- Decouple code execution.
- Postponed task run.
- Periodic task run.
- Load distribution for heavy tasks.

Implementation is simple (compared to standalone services), but enougth for
the most of projects. Advantage of this package is the ease of use.


## API

### new Queue(redis, prefix)

 - **redis** (RedisClient) - redis client instance
 - **prefix** (String) - optional. Redis keys prefix, "queue:" by default


### .registerWorker(options) / .registerWorker(name [, cron], process)

Options:

 - **name** (String) - the worker's name
 - **taskID** (Function) - optional, should return new task id. Needed only for
   creating "exclusive" tasks, return random value by default, called as:
   `function (taskData)`. Sugar: if you pass plain string, it will be wrapped to
   function, that always return this string.
 - **chunksPerInstance** (Number) - optional, available count of parallel chunks
   in one process (Infinity - not restricted), default Infinity
 - **retry** (Number) - optional, number of retry on error, default 2
 - **retryDelay** (Number) - optional, delay in ms after retries, default 60000 ms
 - **timeout** (Number) - optional, `map`, `chunk` and `reduce` execution
   timeout, default 30000 ms
 - **postponeDelay** (Number) - optional, if postpone is called without delay,
   delay is assumed to be equal to this value (in milliseconds).
 - **cron** (String) - optional, cron string ("15 \*/6 \* \* \*"), default null
 - **noTrack** (Boolean) - By default queue tracks all scheduled tasks, to avoid
   rerun if several servers in cluster have wrong clocks. Locks list is stored for
   3 days. But that can take a lot of memory, if task is scheduled too often (ping
   every second, for example). Set `noTrack` = true to disable such tracking.
 - **map** (Function) - optional, proxy taskData to single chunk by default,
   called as: `task.map(callback)`
   - **this** (Object) - current task (task data is available as `this.data`)
   - **callback** (Function) - called as: `function (err, chunksData)`
     - **chunksData** (Array) - array of chunks data
 - **process** (Function) - called as: `chunk.process(callback)`
   - **this** (Object) - current chunk (chunk data is available as `this.data`)
   - **callback** (Function) - called as: `function (err, result)`
 - **reduce** (Function) - optional, only call `callback` by default,
   called as: `task.reduce(chunksResult, callback)`
   - **this** (Object) - current task
   - **chunksResult** (Array) - array of chunks results
   - **callback** (Function) - called as: `function (err)`

We use classic map/reduce approach to run tasks. Each task is mapped to chunks,
chunks are executed in parallel on all available queue nodes, then collected data
is reduced to result.

- If map/process/reduce return error or is timed out, this step is repeaded
  after delay. _Be careful - don't return error on bad data, create a new task
  to process again. Errors are for global failures (redis, db and so on)._
- You can configure number of chunks to run in parallel on each node (with
  `chunksPerInstance`). For example, IO-intensive chunks like sending emails
   need no limits. CPU-intensive chunks should be limited.


#### `map` function

This function is called when you create a new task, and it's purpose is to split
input data into data chunks. By default it creates a single chunk containing
input data as is.

`this` object represents a task. Structure:

 - **id** (String) - task ID, generated by `worker.taskID` on task creation
 - **data** (Object) - input data
 - **worker** (Object) - a worker instance this task corresponds to (see options above)


#### `process` function

This function is called once for each chunk returned from a `map` function.

`this` object represents a chunk:

 - **id** (String) - randomly generated chunk ID
 - **task** (Object) - a task this chunk corresponds to
   - **id** (String) - task ID, generated by `worker.taskID` on task creation
   - **worker** (Object) - a worker instance this task corresponds to (see options above)


#### `reduce` function

This function is called after all chunks are processed. This is the last step of the task execution.

`this` object represents a task:

 - **id** (String) - task ID, generated by `worker.taskID` on task creation
 - **worker** (Object) - a worker instance this task corresponds to (see options above)


### .start()

Queue is stopped by default, because it doesn't know how to process persistent
data until you register workers. You should call `.start()` after queue is
configured.

__Note.__ It may happen, that you remove some worker from your app. In this case
orphaned data will be wiped after 3 days.


### .shutdown()

Stop accepting new tasks (and chunks) from queue.

__Note.__ It may take some time until all activities are compleate. That includes:

- tasks in mapping phase
- tasks in aggregating phase
- active chunks


### .on('error', callback)

`Queue` is an `EventEmitter` instance that only fires `error` event when
an error has occured.


### .worker(name)

Get worker by name. Returns `null` if worker not exists.


### .worker(name).push([taskData,] callback)

Create new task and run it immediately.

 - **taskData** (Object) - optional, the task params
 - **callback** (Function) - called as: `function (err, task_id)`

If worker has custom `.taskID()`, then duplicated tasks will be ignored silently.

### .worker(name).postpone([taskData, delay,] callback)

Create new task and postpone execution to given delay.

 - **taskData** (Object) - optional, the task params, default: `null`
 - **delay** (Number) - optional, delay execution for the given amount of time,
   default: `worker.postponeDelay`
 - **callback** (Function) - called as: `function (err, task_id)`

If worker has custom `.taskID()`, then

- duplicated tasks will replace pending one (with prolonged delay)
- if queue needs to run postponed task, but duplicated one is already active,
  then queue will retry attempts on new ticks (after 0.5s) until succeed.

__Note.__ If you call `.postpone()` with 2 arguments, queue figures out desired
signature based on argument type (if it's a number, it's a delay). Thus,
if you wish to supply a numeric `taskData`, you *must* also specify `delay`.


### .worker(name).status([taskID,] callback)

Get information about task status.

 - **taskID** (String) - optional, task ID returned from `push` or `postpone`
   functions, result of `.taskID()` by default
 - **callback** (Function) - called as `function (err, info)`
   - **info** (Object | Null) - task info if task exists, null otherwise
     - **worker** (String) - worker name
     - **state**  (String) - one of "mapping", "aggregating", "reducing" or
       "postponed"
     - **chunks** (Object) - chunk counts by state (optional, only present
       if task is in "aggregating" state)
       - **pending** - count of pending chunks
       - **active**  - count of active chunks
       - **done**    - count of completed chunks
       - **errored** - count of failed chunks


### .worker(name).cancel([taskID,] callback)

Cancel the task and remove it from queue. Chunks that started execution
will continue, but their results will be discarded and no new chunks
will be processed.

 - **taskID** (String) - optional, task ID returned from `push` or `postpone`
   functions, result of `.taskID()` by default
 - **callback** (Function) - called as: `function (err)`


### .worker(name).taskID([taskData])

Generates task ID by task data. By default returns random string. Queue requires
all tasks to be unique, and reject duplicating ones. So, you can customize
`taskID()` on worker registration for specific worker behaviour. For example,
you can return constant to allow only single task instance to run.


### chunk.setDeadline([delay, callback])

All chunks have a limited time to finish (timeout). If not complete, those will
be considered as failed. But sometime you may need to extend default deadline.
This can be done by `.setDeadline()` method of chunk.

- Default timeout is defined on forker registration (30s if not set).
- If `delay` param is not passed, default timeout will be used.
